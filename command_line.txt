python3 main.py \
    --use_gpu \
    --model "baseline" \
    --model_path "squad_model.pt" \
    --train_path "datasets/squad_train.jsonl.gz" \
    --dev_path "datasets/squad_dev.jsonl.gz" \
    --output_path "squad_predictions.txt" \
    --hidden_dim 256 \
    --bidirectional \
    --do_train \
    --do_test

python3 main.py \
    --model "baseline" \
    --model_path "squad_model.pt" \
    --train_path "datasets/squad_train.jsonl.gz" \
    --dev_path "datasets/squad_dev.jsonl.gz" \
    --output_path "squad_predictions.txt" \
    --hidden_dim 256 \
    --bidirectional \
    --do_train \
    --do_test

python3 visualize.py --path datasets/squad_train.jsonl.gz --samples 0

Trying to find word overlap between question and passage, but it often messes up if there is a high word overlap between question and
context. If you take pretrained model and give it a passage where words overlap in one area even though the answer is somewhere else.
Might need coreference system to see that 'he' refers to 'Barack Obama.'

LSTM is kind of creating a collective embedding for passage and question. Then SpanAttention happens where it looks for the "meat"
of the question. Attention is doing most of the work. SpanAttention is finding key words inside of the question.

About shortening the context which you then run the QA model

NER is to decouple the procedure into two steps. Find relevant context from the context. NER can find instances of name in question 
within the passage.
Dependency parsing can provide structure

Could be unrelated to model used to find pruned context. Want to show that two-stage pipeline would help.

Might not work on pretrained model because it might be trained to home in on things that aren't relevant anymore.

It could be potentially advantageous to fine-tune a pretrained model over training from scratch.

QADataset create_samples gets the stuff we actually need

lookup whitespace tokenizer in spacy to disable extra stuff (will work b/c there will be a space between punctuation)

need to make sure vocab is correct